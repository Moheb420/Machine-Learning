{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## Hyperparameters and Gridsearch \n",
    "\n",
    "When instantiating a Scikit-learn model in python most or all constructor parameters have _default_ values. These values are not part of the internal model and are hence called ___hyperparameters___---in contrast to _normal_ model parameters, for example the neuron weights, $\\mathbf w$, for an `MLP` model.\n",
    "\n",
    "### Manual Tuning Hyperparameters\n",
    "\n",
    "Below is an example of the python constructor for the support-vector classifier `sklearn.svm.SVC`, with say the `kernel` hyperparameter having the default value `'rbf'`. If you should choose, what would you set it to other than `'rbf'`? \n",
    "\n",
    "```python\n",
    "class sklearn.svm.SVC(\n",
    "    C=1.0, \n",
    "    kernel=’rbf’, \n",
    "    degree=3,\n",
    "    gamma=’auto_deprecated’, \n",
    "    coef0=0.0, \n",
    "    shrinking=True, \n",
    "    probability=False, \n",
    "    tol=0.001, \n",
    "    cache_size=200, \n",
    "    class_weight=None, \n",
    "    verbose=False, \n",
    "    max_iter=-1, \n",
    "    decision_function_shape=’ovr’, \n",
    "    random_state=None\n",
    "  )\n",
    "```  \n",
    "\n",
    "The default values might be a sensible general starting point, but for your data, you might want to optimize the hyperparameters to yield a better result. \n",
    "\n",
    "To be able to set `kernel` to a sensible value you need to go into the documentation for the `SVC` and understand what the kernel parameter represents, and what values it can be set to, and you need to understand the consequences of setting `kernel` to something different than the default...and the story repeats for every other hyperparameter!\n",
    "\n",
    "### Brute Force  Search\n",
    "\n",
    "An alternative to this structured, but time-consuming approach, is just to __brute-force__ a search of interesting hyperparameters, and  choose the 'best' parameters according to a fit-predict and some score, say 'f1'. \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L09/Figs/gridsearch.png\"  alt=\"WARNING: could not get image from server.\"  style=\"width:350px\">\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of grid search for two distinct hyperparameters. </center> \n",
    "    <center> Notice that you would normally search hyperparameters like `alpha` with an exponential range, say [0.01, 0.1, 1, 10] or similar.</center>\n",
    "</em></small>\n",
    "\n",
    "Now, you just pick out some hyperparameters, that you figure are important, set them to a suitable range, say\n",
    "\n",
    "```python\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 10]\n",
    "```\n",
    "and fire up a full (grid) search on this hyperparameter set, that will try out all your specified combination of `kernel` and `C` for the model, and then prints the hyperparameter set with the highest score...\n",
    "\n",
    "The demo code below sets up some of our well known 'hello-world' data and then run a _grid search_ on a particular model, here a _support-vector classifier_ (SVC)\n",
    "\n",
    "Other models and datasets  ('mnist', 'iris', 'moon') can also be examined.\n",
    "\n",
    "### Qa Explain GridSearchCV\n",
    "\n",
    "There are two code cells below: 1) function setup, 2) the actual grid-search.\n",
    "\n",
    "Review the code cells and write a __short__ summary. Mainly focus on __cell 2__, but dig into cell 1 if you find it interesting (notice the use of local-function, a nifty feature in python).\n",
    "  \n",
    "In detail, examine the lines:  \n",
    "  \n",
    "```python\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, ..\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "..\n",
    "FullReport(grid_tuned , X_test, y_test, time_gridsearch)\n",
    "```\n",
    "and write a short description of how the `GridSeachCV` works: explain how the search parameter set is created and the overall search mechanism is functioning (without going into too much detail).\n",
    "\n",
    "What role does the parameter `scoring='f1_micro'` play in the `GridSearchCV`, and what does `n_jobs=-1` mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK(function setup, hope MNIST loads works because it seems you miss the installation of Keras or Tensorflow!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: importing 'keras' failed\n",
      "WARNING: importing 'tensorflow.keras' failed\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 1) function setup\n",
    "\n",
    "from time import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "from libitmal import dataloaders as itmaldataloaders # Needed for load of iris, moon and mnist\n",
    "\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            ret_str=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                temp_str = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(ret_str)>0:\n",
    "                    ret_str += ','\n",
    "                ret_str += f'{key}={temp_str}{value}{temp_str}'  \n",
    "            return ret_str          \n",
    "        try:\n",
    "            param_str = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + param_str + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    print()\n",
    "    \n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "    \n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "    \n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "    \n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(load_mode=0)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}', only 'moon'/'mnist'/'iris' supported\")\n",
    "        \n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def TryKerasImport(verbose=True):\n",
    "    \n",
    "    kerasok = True\n",
    "    try:\n",
    "        import keras as keras_try\n",
    "    except:\n",
    "        kerasok = False\n",
    "\n",
    "    tensorflowkerasok = True\n",
    "    try:\n",
    "        import tensorflow.keras as tensorflowkeras_try\n",
    "    except:\n",
    "        tensorflowkerasok = False\n",
    "        \n",
    "    ok = kerasok or tensorflowkerasok\n",
    "    \n",
    "    if not ok and verbose:\n",
    "        if not kerasok:\n",
    "            print(\"WARNING: importing 'keras' failed\", file=sys.stderr)\n",
    "        if not tensorflowkerasok:\n",
    "            print(\"WARNING: importing 'tensorflow.keras' failed\", file=sys.stderr)\n",
    "\n",
    "    return ok\n",
    "    \n",
    "print(f\"OK(function setup\" + (\"\" if TryKerasImport() else \", hope MNIST loads works because it seems you miss the installation of Keras or Tensorflow!\") + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "SEARCH TIME: 3.23 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'C': 1, 'kernel': 'linear'}\n",
      "\tbest 'f1_micro' score=0.9714285714285715\n",
      "\tbest index=2\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.962 (+/-0.093) for {'C': 0.1, 'kernel': 'linear'}\n",
      "\t[ 1]: 0.371 (+/-0.038) for {'C': 0.1, 'kernel': 'rbf'}\n",
      "\t[ 2]: 0.971 (+/-0.047) for {'C': 1, 'kernel': 'linear'}\n",
      "\t[ 3]: 0.695 (+/-0.047) for {'C': 1, 'kernel': 'rbf'}\n",
      "\t[ 4]: 0.952 (+/-0.085) for {'C': 10, 'kernel': 'linear'}\n",
      "\t[ 5]: 0.924 (+/-0.097) for {'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "CTOR for best model: SVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "best: dat=iris, score=0.97143, model=SVC(C=1,kernel='linear')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa, code review..cell 2) the actual grid-search\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = svm.SVC(\n",
    "    gamma=0.001\n",
    ")  # NOTE: gamma=\"scale\" does not work in older Scikit-learn frameworks,\n",
    "# FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'kernel': ('linear', 'rbf'), \n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb Hyperparameter Grid Search using an SDG classifier\n",
    "\n",
    "Now, replace the `svm.SVC` model with an `SGDClassifier` and a suitable set of the hyperparameters for that model.\n",
    "\n",
    "You need at least four or five different hyperparameters from the `SGDClassifier` in the search-space before it begins to take considerable compute time doing the full grid search.\n",
    "\n",
    "So, repeat the search with the `SGDClassifier`, and be sure to add enough hyperparameters to the grid-search, such that the search takes a considerable time to run, that is a couple of minutes or up to some hours.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "SEARCH TIME: 5.35 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\tbest 'f1_micro' score=1.0\n",
      "\tbest index=494\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.01, eta0=1, penalty='l1')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[ 1]: 0.857 (+/-0.159) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[ 2]: 0.857 (+/-0.217) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[ 3]: 0.943 (+/-0.093) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[ 4]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[ 5]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[ 6]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[ 7]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[ 8]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[ 9]: 0.810 (+/-0.209) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[10]: 0.800 (+/-0.258) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[11]: 0.829 (+/-0.177) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[12]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[13]: 0.724 (+/-0.229) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[14]: 0.857 (+/-0.060) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[15]: 0.867 (+/-0.220) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[16]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[17]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[18]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[19]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[20]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[21]: 0.600 (+/-0.214) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[22]: 0.838 (+/-0.322) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[23]: 0.790 (+/-0.222) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[24]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[25]: 0.695 (+/-0.047) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[26]: 0.695 (+/-0.047) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[27]: 0.695 (+/-0.047) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[28]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[29]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[30]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[31]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[32]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[33]: 0.800 (+/-0.194) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[34]: 0.829 (+/-0.129) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[35]: 0.819 (+/-0.126) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[36]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[37]: 0.895 (+/-0.071) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[38]: 0.914 (+/-0.038) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[39]: 0.905 (+/-0.060) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[40]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[41]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[42]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[43]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[44]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[45]: 0.971 (+/-0.076) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[46]: 0.990 (+/-0.038) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[47]: 0.981 (+/-0.047) for {'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[48]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[49]: 0.781 (+/-0.322) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[50]: 0.771 (+/-0.229) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[51]: 0.829 (+/-0.214) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[52]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[53]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[54]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[55]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[56]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[57]: 0.752 (+/-0.291) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[58]: 0.638 (+/-0.196) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[59]: 0.610 (+/-0.388) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[60]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[61]: 0.771 (+/-0.265) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[62]: 0.838 (+/-0.129) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[63]: 0.933 (+/-0.114) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[64]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[65]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[66]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[67]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[68]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[69]: 0.743 (+/-0.129) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[70]: 0.829 (+/-0.196) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[71]: 0.771 (+/-0.358) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[72]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[73]: 0.819 (+/-0.093) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[74]: 0.800 (+/-0.229) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[75]: 0.810 (+/-0.159) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[76]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[77]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[78]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[79]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[80]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[81]: 0.971 (+/-0.076) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[82]: 0.895 (+/-0.164) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[83]: 0.952 (+/-0.060) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[84]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[85]: 0.962 (+/-0.071) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[86]: 0.971 (+/-0.047) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[87]: 0.971 (+/-0.047) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[88]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[89]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[90]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[91]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[92]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[93]: 0.952 (+/-0.060) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[94]: 0.943 (+/-0.093) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[95]: 0.943 (+/-0.071) for {'alpha': 0.0001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[96]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[97]: 0.733 (+/-0.506) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[98]: 0.762 (+/-0.217) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[99]: 0.695 (+/-0.457) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[100]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[101]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[102]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[103]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[104]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[105]: 0.676 (+/-0.368) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[106]: 0.762 (+/-0.241) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[107]: 0.657 (+/-0.448) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[108]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[109]: 0.781 (+/-0.166) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[110]: 0.886 (+/-0.214) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[111]: 0.829 (+/-0.245) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[112]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[113]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[114]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[115]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[116]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[117]: 0.905 (+/-0.170) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[118]: 0.857 (+/-0.181) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[119]: 0.762 (+/-0.381) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[120]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[121]: 0.905 (+/-0.148) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[122]: 0.952 (+/-0.060) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[123]: 0.914 (+/-0.071) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[124]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[125]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[126]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[127]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[128]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[129]: 0.971 (+/-0.047) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[130]: 0.943 (+/-0.111) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[131]: 0.895 (+/-0.185) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[132]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[133]: 0.943 (+/-0.093) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[134]: 0.943 (+/-0.071) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[135]: 0.943 (+/-0.071) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[136]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[137]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[138]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[139]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[140]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[141]: 0.924 (+/-0.129) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[142]: 0.924 (+/-0.129) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[143]: 0.952 (+/-0.060) for {'alpha': 0.0001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[144]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[145]: 0.648 (+/-0.222) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[146]: 0.714 (+/-0.269) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[147]: 0.705 (+/-0.152) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[148]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[149]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[150]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[151]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[152]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[153]: 0.600 (+/-0.245) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[154]: 0.676 (+/-0.363) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[155]: 0.667 (+/-0.413) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[156]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[157]: 0.781 (+/-0.196) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[158]: 0.857 (+/-0.159) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[159]: 0.752 (+/-0.185) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[160]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[161]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[162]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[163]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[164]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[165]: 0.867 (+/-0.251) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[166]: 0.762 (+/-0.346) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[167]: 0.771 (+/-0.185) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[168]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[169]: 0.933 (+/-0.177) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[170]: 0.905 (+/-0.135) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[171]: 0.943 (+/-0.093) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[172]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[173]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[174]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[175]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[176]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[177]: 0.895 (+/-0.220) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[178]: 0.886 (+/-0.076) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[179]: 0.914 (+/-0.164) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[180]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[181]: 0.962 (+/-0.071) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[182]: 0.962 (+/-0.071) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[183]: 0.933 (+/-0.129) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[184]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[185]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[186]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[187]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[188]: nan (+/-nan) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[189]: 0.933 (+/-0.129) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[190]: 0.952 (+/-0.104) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[191]: 0.952 (+/-0.104) for {'alpha': 0.0001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[192]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[193]: 0.867 (+/-0.185) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[194]: 0.886 (+/-0.177) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[195]: 0.857 (+/-0.209) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[196]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[197]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[198]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[199]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[200]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[201]: 0.781 (+/-0.245) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[202]: 0.819 (+/-0.175) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[203]: 0.771 (+/-0.185) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[204]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[205]: 0.800 (+/-0.220) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[206]: 0.952 (+/-0.060) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[207]: 0.895 (+/-0.164) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[208]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[209]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[210]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[211]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[212]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[213]: 0.914 (+/-0.185) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[214]: 0.905 (+/-0.085) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[215]: 0.810 (+/-0.241) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[216]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[217]: 0.695 (+/-0.047) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[218]: 0.695 (+/-0.047) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[219]: 0.695 (+/-0.047) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[220]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[221]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[222]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[223]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[224]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[225]: 0.810 (+/-0.135) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[226]: 0.848 (+/-0.038) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[227]: 0.829 (+/-0.166) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[228]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[229]: 0.905 (+/-0.060) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[230]: 0.905 (+/-0.060) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[231]: 0.933 (+/-0.076) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[232]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[233]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[234]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[235]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[236]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[237]: 0.981 (+/-0.047) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[238]: 0.981 (+/-0.047) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[239]: 0.971 (+/-0.047) for {'alpha': 0.001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[240]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[241]: 0.819 (+/-0.220) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[242]: 0.829 (+/-0.196) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[243]: 0.714 (+/-0.085) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[244]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[245]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[246]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[247]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[248]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[249]: 0.619 (+/-0.341) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[250]: 0.657 (+/-0.229) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[251]: 0.819 (+/-0.258) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[252]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[253]: 0.838 (+/-0.273) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[254]: 0.962 (+/-0.071) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[255]: 0.905 (+/-0.159) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[256]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[257]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[258]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[259]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[260]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[261]: 0.781 (+/-0.333) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[262]: 0.924 (+/-0.214) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[263]: 0.867 (+/-0.194) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[264]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[265]: 0.838 (+/-0.187) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[266]: 0.819 (+/-0.164) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[267]: 0.800 (+/-0.140) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[268]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[269]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[270]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[271]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[272]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[273]: 0.924 (+/-0.076) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[274]: 0.876 (+/-0.177) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[275]: 0.943 (+/-0.111) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[276]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[277]: 0.971 (+/-0.047) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[278]: 0.981 (+/-0.047) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[279]: 0.971 (+/-0.047) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[280]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[281]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[282]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[283]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[284]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[285]: 0.952 (+/-0.060) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[286]: 0.971 (+/-0.047) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[287]: 0.933 (+/-0.097) for {'alpha': 0.001, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[288]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[289]: 0.610 (+/-0.285) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[290]: 0.743 (+/-0.379) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[291]: 0.638 (+/-0.388) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[292]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[293]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[294]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[295]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[296]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[297]: 0.590 (+/-0.547) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[298]: 0.695 (+/-0.441) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[299]: 0.752 (+/-0.194) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[300]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[301]: 0.886 (+/-0.143) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[302]: 0.905 (+/-0.148) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[303]: 0.848 (+/-0.194) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[304]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[305]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[306]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[307]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[308]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[309]: 0.924 (+/-0.129) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[310]: 0.924 (+/-0.129) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[311]: 0.914 (+/-0.251) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[312]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[313]: 0.981 (+/-0.047) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[314]: 0.933 (+/-0.076) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[315]: 0.895 (+/-0.126) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[316]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[317]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[318]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[319]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[320]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[321]: 0.952 (+/-0.060) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[322]: 0.952 (+/-0.060) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[323]: 0.914 (+/-0.203) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[324]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[325]: 0.952 (+/-0.104) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[326]: 0.952 (+/-0.060) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[327]: 0.943 (+/-0.093) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[328]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[329]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[330]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[331]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[332]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[333]: 0.952 (+/-0.060) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[334]: 0.943 (+/-0.071) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[335]: 0.933 (+/-0.097) for {'alpha': 0.001, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[336]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[337]: 0.438 (+/-0.383) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[338]: 0.705 (+/-0.038) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[339]: 0.638 (+/-0.322) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[340]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[341]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[342]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[343]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[344]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[345]: 0.533 (+/-0.332) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[346]: 0.752 (+/-0.203) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[347]: 0.629 (+/-0.203) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[348]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[349]: 0.781 (+/-0.222) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[350]: 0.952 (+/-0.104) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[351]: 0.829 (+/-0.322) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[352]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[353]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[354]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[355]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[356]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[357]: 0.838 (+/-0.222) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[358]: 0.962 (+/-0.038) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[359]: 0.895 (+/-0.093) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[360]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[361]: 0.867 (+/-0.175) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[362]: 0.943 (+/-0.093) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[363]: 0.914 (+/-0.071) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[364]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[365]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[366]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[367]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[368]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[369]: 0.905 (+/-0.200) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[370]: 0.924 (+/-0.155) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[371]: 0.914 (+/-0.126) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[372]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[373]: 0.952 (+/-0.104) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[374]: 0.952 (+/-0.060) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[375]: 0.943 (+/-0.093) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[376]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[377]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[378]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[379]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[380]: nan (+/-nan) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[381]: 0.943 (+/-0.140) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[382]: 0.943 (+/-0.140) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[383]: 0.933 (+/-0.129) for {'alpha': 0.001, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[384]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[385]: 0.857 (+/-0.313) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[386]: 0.838 (+/-0.196) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[387]: 0.838 (+/-0.222) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[388]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[389]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[390]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[391]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[392]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[393]: 0.752 (+/-0.203) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[394]: 0.886 (+/-0.196) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[395]: 0.819 (+/-0.203) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[396]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[397]: 0.857 (+/-0.200) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[398]: 0.971 (+/-0.047) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[399]: 0.867 (+/-0.175) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[400]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[401]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[402]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[403]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[404]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[405]: 0.848 (+/-0.229) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[406]: 0.962 (+/-0.038) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[407]: 0.924 (+/-0.129) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[408]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[409]: 0.695 (+/-0.047) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[410]: 0.695 (+/-0.047) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[411]: 0.695 (+/-0.047) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[412]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[413]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[414]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[415]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[416]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[417]: 0.819 (+/-0.140) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[418]: 0.771 (+/-0.185) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[419]: 0.857 (+/-0.135) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[420]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[421]: 0.895 (+/-0.071) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[422]: 0.886 (+/-0.076) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[423]: 0.876 (+/-0.047) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[424]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[425]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[426]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[427]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[428]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[429]: 0.952 (+/-0.148) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[430]: 0.990 (+/-0.038) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[431]: 0.981 (+/-0.047) for {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[432]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[433]: 0.790 (+/-0.280) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[434]: 0.838 (+/-0.155) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[435]: 0.686 (+/-0.047) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[436]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[437]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[438]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[439]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[440]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[441]: 0.667 (+/-0.200) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[442]: 0.867 (+/-0.185) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[443]: 0.505 (+/-0.299) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[444]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[445]: 0.876 (+/-0.129) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[446]: 0.990 (+/-0.038) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[447]: 0.848 (+/-0.185) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[448]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[449]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[450]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[451]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[452]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[453]: 0.848 (+/-0.220) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[454]: 0.990 (+/-0.038) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[455]: 0.971 (+/-0.114) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[456]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[457]: 0.848 (+/-0.038) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[458]: 0.838 (+/-0.129) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[459]: 0.848 (+/-0.071) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[460]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[461]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[462]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[463]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[464]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[465]: 0.933 (+/-0.076) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[466]: 0.876 (+/-0.230) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[467]: 0.933 (+/-0.047) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[468]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[469]: 0.962 (+/-0.071) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[470]: 0.990 (+/-0.038) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[471]: 0.990 (+/-0.038) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[472]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[473]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[474]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[475]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[476]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[477]: 0.943 (+/-0.071) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[478]: 0.952 (+/-0.060) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[479]: 0.933 (+/-0.097) for {'alpha': 0.01, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[480]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[481]: 0.600 (+/-0.441) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[482]: 0.648 (+/-0.273) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[483]: 0.410 (+/-0.267) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[484]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[485]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[486]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[487]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[488]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[489]: 0.581 (+/-0.338) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[490]: 0.733 (+/-0.364) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[491]: 0.657 (+/-0.406) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[492]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[493]: 0.867 (+/-0.212) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[494]: 1.000 (+/-0.000) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[495]: 0.810 (+/-0.217) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[496]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[497]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[498]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[499]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[500]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[501]: 0.924 (+/-0.177) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[502]: 0.971 (+/-0.047) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[503]: 0.933 (+/-0.076) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[504]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[505]: 0.933 (+/-0.129) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[506]: 0.905 (+/-0.085) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[507]: 0.886 (+/-0.076) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[508]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[509]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[510]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[511]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[512]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[513]: 0.810 (+/-0.241) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[514]: 0.933 (+/-0.143) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[515]: 0.971 (+/-0.047) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[516]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[517]: 0.924 (+/-0.166) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[518]: 0.962 (+/-0.038) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[519]: 0.924 (+/-0.166) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[520]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[521]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[522]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[523]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[524]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[525]: 0.943 (+/-0.140) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[526]: 0.914 (+/-0.111) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[527]: 0.943 (+/-0.140) for {'alpha': 0.01, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[528]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[529]: 0.400 (+/-0.322) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[530]: 0.705 (+/-0.423) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[531]: 0.429 (+/-0.335) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[532]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[533]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[534]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[535]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[536]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[537]: 0.438 (+/-0.258) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[538]: 0.705 (+/-0.071) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[539]: 0.495 (+/-0.359) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[540]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[541]: 0.838 (+/-0.205) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[542]: 0.981 (+/-0.047) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[543]: 0.952 (+/-0.085) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[544]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[545]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[546]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[547]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[548]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[549]: 0.905 (+/-0.159) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[550]: 0.981 (+/-0.047) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[551]: 0.924 (+/-0.129) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[552]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[553]: 0.552 (+/-0.359) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[554]: 0.905 (+/-0.104) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[555]: 0.657 (+/-0.291) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[556]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[557]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[558]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[559]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[560]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[561]: 0.667 (+/-0.256) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[562]: 0.905 (+/-0.200) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[563]: 0.657 (+/-0.406) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[564]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[565]: 0.886 (+/-0.143) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[566]: 0.971 (+/-0.047) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[567]: 0.905 (+/-0.104) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[568]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[569]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[570]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[571]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[572]: nan (+/-nan) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[573]: 0.876 (+/-0.129) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[574]: 0.933 (+/-0.129) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[575]: 0.876 (+/-0.129) for {'alpha': 0.01, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[576]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[577]: 0.743 (+/-0.214) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[578]: 0.705 (+/-0.038) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[579]: 0.705 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[580]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[581]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[582]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[583]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[584]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[585]: 0.714 (+/-0.301) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[586]: 0.857 (+/-0.256) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[587]: 0.733 (+/-0.129) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[588]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[589]: 0.829 (+/-0.177) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[590]: 0.781 (+/-0.155) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[591]: 0.819 (+/-0.194) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[592]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[593]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[594]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[595]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[596]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[597]: 0.914 (+/-0.111) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[598]: 0.981 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[599]: 0.943 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[600]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[601]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[602]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[603]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[604]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[605]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[606]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[607]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[608]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[609]: 0.781 (+/-0.143) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[610]: 0.743 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[611]: 0.743 (+/-0.129) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[612]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[613]: 0.743 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[614]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[615]: 0.705 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[616]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[617]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[618]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[619]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[620]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[621]: 0.962 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[622]: 0.943 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[623]: 0.962 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[624]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[625]: 0.686 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[626]: 0.657 (+/-0.071) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[627]: 0.686 (+/-0.424) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[628]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[629]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[630]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[631]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[632]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[633]: 0.619 (+/-0.301) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[634]: 0.743 (+/-0.524) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[635]: 0.590 (+/-0.322) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[636]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[637]: 0.743 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[638]: 0.762 (+/-0.200) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[639]: 0.819 (+/-0.220) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[640]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[641]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[642]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[643]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[644]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[645]: 0.914 (+/-0.093) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[646]: 0.952 (+/-0.060) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[647]: 0.933 (+/-0.129) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[648]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[649]: 0.733 (+/-0.177) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[650]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[651]: 0.733 (+/-0.129) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[652]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[653]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[654]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[655]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[656]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[657]: 0.933 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[658]: 0.838 (+/-0.222) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[659]: 0.829 (+/-0.245) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[660]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[661]: 0.829 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[662]: 0.752 (+/-0.093) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[663]: 0.790 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[664]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[665]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[666]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[667]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[668]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[669]: 0.943 (+/-0.093) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[670]: 0.971 (+/-0.076) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[671]: 0.933 (+/-0.097) for {'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[672]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[673]: 0.390 (+/-0.164) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[674]: 0.733 (+/-0.299) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[675]: 0.524 (+/-0.289) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[676]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[677]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[678]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[679]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[680]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[681]: 0.362 (+/-0.177) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[682]: 0.676 (+/-0.392) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[683]: 0.571 (+/-0.356) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[684]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[685]: 0.838 (+/-0.260) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[686]: 0.790 (+/-0.047) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[687]: 0.800 (+/-0.140) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[688]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[689]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[690]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[691]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[692]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[693]: 0.914 (+/-0.093) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[694]: 0.981 (+/-0.047) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[695]: 0.867 (+/-0.203) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[696]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[697]: 0.743 (+/-0.293) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[698]: 0.705 (+/-0.038) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[699]: 0.733 (+/-0.097) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[700]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[701]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[702]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[703]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[704]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[705]: 0.533 (+/-0.298) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[706]: 0.981 (+/-0.047) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[707]: 0.714 (+/-0.085) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[708]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[709]: 0.886 (+/-0.129) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[710]: 0.924 (+/-0.076) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[711]: 0.886 (+/-0.097) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[712]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[713]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[714]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[715]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[716]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[717]: 0.895 (+/-0.140) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[718]: 0.962 (+/-0.111) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[719]: 0.886 (+/-0.097) for {'alpha': 0.1, 'eta0': 1, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[720]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[721]: 0.333 (+/-0.085) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[722]: 0.714 (+/-0.459) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[723]: 0.324 (+/-0.071) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[724]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[725]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[726]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[727]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[728]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[729]: 0.343 (+/-0.071) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[730]: 0.695 (+/-0.047) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[731]: 0.352 (+/-0.047) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'constant', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[732]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[733]: 0.771 (+/-0.203) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[734]: 0.895 (+/-0.244) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[735]: 0.838 (+/-0.155) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[736]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[737]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[738]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[739]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[740]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[741]: 0.905 (+/-0.159) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[742]: 0.981 (+/-0.047) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[743]: 0.848 (+/-0.220) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'optimal', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[744]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[745]: 0.619 (+/-0.289) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[746]: 0.790 (+/-0.143) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[747]: 0.648 (+/-0.393) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[748]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[749]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[750]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[751]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[752]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[753]: 0.590 (+/-0.393) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[754]: 0.952 (+/-0.120) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[755]: 0.571 (+/-0.486) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'invscaling', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\t[756]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'none'}\n",
      "\t[757]: 0.829 (+/-0.155) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
      "\t[758]: 0.943 (+/-0.071) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l1'}\n",
      "\t[759]: 0.876 (+/-0.076) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'elasticnet'}\n",
      "\t[760]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'none'}\n",
      "\t[761]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l2'}\n",
      "\t[762]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'l1'}\n",
      "\t[763]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'log', 'penalty': 'elasticnet'}\n",
      "\t[764]: nan (+/-nan) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'none'}\n",
      "\t[765]: 0.857 (+/-0.148) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\t[766]: 0.952 (+/-0.104) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'l1'}\n",
      "\t[767]: 0.886 (+/-0.076) for {'alpha': 0.1, 'eta0': 10, 'learning_rate': 'adaptive', 'loss': 'modified_huber', 'penalty': 'elasticnet'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.79      1.00      0.88        11\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.01, eta0=1, penalty='l1')\n",
      "\n",
      "best: dat=iris, score=1.00000, model=SGDClassifier(alpha=0.01,eta0=1,learning_rate='optimal',loss='hinge',penalty='l1')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1920 fits failed out of a total of 3840.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "170 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of SGDClassifier must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "289 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of SGDClassifier must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "84 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of SGDClassifier must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "168 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_error', 'squared_hinge', 'hinge', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'huber', 'log_loss', 'modified_huber', 'perceptron'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "204 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'huber', 'squared_hinge', 'epsilon_insensitive', 'hinge', 'squared_error', 'squared_epsilon_insensitive', 'perceptron', 'log_loss'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "176 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'huber', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_epsilon_insensitive', 'log_loss', 'squared_error', 'epsilon_insensitive', 'hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "144 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'epsilon_insensitive', 'perceptron', 'huber', 'squared_hinge', 'modified_huber', 'squared_error', 'log_loss', 'squared_epsilon_insensitive'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "146 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_error', 'perceptron', 'huber', 'squared_hinge', 'log_loss', 'modified_huber', 'squared_epsilon_insensitive', 'epsilon_insensitive'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "174 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'squared_hinge', 'perceptron', 'log_loss', 'hinge', 'huber', 'epsilon_insensitive', 'squared_error', 'modified_huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "97 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of SGDClassifier must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "144 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'modified_huber', 'hinge', 'squared_error', 'squared_hinge', 'huber', 'squared_epsilon_insensitive', 'log_loss', 'perceptron'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "124 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'modified_huber', 'perceptron', 'hinge', 'epsilon_insensitive', 'huber', 'squared_error', 'squared_epsilon_insensitive', 'log_loss'}. Got 'log' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan 0.85714286 0.85714286 0.94285714        nan        nan\n",
      "        nan        nan        nan 0.80952381 0.8        0.82857143\n",
      "        nan 0.72380952 0.85714286 0.86666667        nan        nan\n",
      "        nan        nan        nan 0.6        0.83809524 0.79047619\n",
      "        nan 0.6952381  0.6952381  0.6952381         nan        nan\n",
      "        nan        nan        nan 0.8        0.82857143 0.81904762\n",
      "        nan 0.8952381  0.91428571 0.9047619         nan        nan\n",
      "        nan        nan        nan 0.97142857 0.99047619 0.98095238\n",
      "        nan 0.78095238 0.77142857 0.82857143        nan        nan\n",
      "        nan        nan        nan 0.75238095 0.63809524 0.60952381\n",
      "        nan 0.77142857 0.83809524 0.93333333        nan        nan\n",
      "        nan        nan        nan 0.74285714 0.82857143 0.77142857\n",
      "        nan 0.81904762 0.8        0.80952381        nan        nan\n",
      "        nan        nan        nan 0.97142857 0.8952381  0.95238095\n",
      "        nan 0.96190476 0.97142857 0.97142857        nan        nan\n",
      "        nan        nan        nan 0.95238095 0.94285714 0.94285714\n",
      "        nan 0.73333333 0.76190476 0.6952381         nan        nan\n",
      "        nan        nan        nan 0.67619048 0.76190476 0.65714286\n",
      "        nan 0.78095238 0.88571429 0.82857143        nan        nan\n",
      "        nan        nan        nan 0.9047619  0.85714286 0.76190476\n",
      "        nan 0.9047619  0.95238095 0.91428571        nan        nan\n",
      "        nan        nan        nan 0.97142857 0.94285714 0.8952381\n",
      "        nan 0.94285714 0.94285714 0.94285714        nan        nan\n",
      "        nan        nan        nan 0.92380952 0.92380952 0.95238095\n",
      "        nan 0.64761905 0.71428571 0.7047619         nan        nan\n",
      "        nan        nan        nan 0.6        0.67619048 0.66666667\n",
      "        nan 0.78095238 0.85714286 0.75238095        nan        nan\n",
      "        nan        nan        nan 0.86666667 0.76190476 0.77142857\n",
      "        nan 0.93333333 0.9047619  0.94285714        nan        nan\n",
      "        nan        nan        nan 0.8952381  0.88571429 0.91428571\n",
      "        nan 0.96190476 0.96190476 0.93333333        nan        nan\n",
      "        nan        nan        nan 0.93333333 0.95238095 0.95238095\n",
      "        nan 0.86666667 0.88571429 0.85714286        nan        nan\n",
      "        nan        nan        nan 0.78095238 0.81904762 0.77142857\n",
      "        nan 0.8        0.95238095 0.8952381         nan        nan\n",
      "        nan        nan        nan 0.91428571 0.9047619  0.80952381\n",
      "        nan 0.6952381  0.6952381  0.6952381         nan        nan\n",
      "        nan        nan        nan 0.80952381 0.84761905 0.82857143\n",
      "        nan 0.9047619  0.9047619  0.93333333        nan        nan\n",
      "        nan        nan        nan 0.98095238 0.98095238 0.97142857\n",
      "        nan 0.81904762 0.82857143 0.71428571        nan        nan\n",
      "        nan        nan        nan 0.61904762 0.65714286 0.81904762\n",
      "        nan 0.83809524 0.96190476 0.9047619         nan        nan\n",
      "        nan        nan        nan 0.78095238 0.92380952 0.86666667\n",
      "        nan 0.83809524 0.81904762 0.8               nan        nan\n",
      "        nan        nan        nan 0.92380952 0.87619048 0.94285714\n",
      "        nan 0.97142857 0.98095238 0.97142857        nan        nan\n",
      "        nan        nan        nan 0.95238095 0.97142857 0.93333333\n",
      "        nan 0.60952381 0.74285714 0.63809524        nan        nan\n",
      "        nan        nan        nan 0.59047619 0.6952381  0.75238095\n",
      "        nan 0.88571429 0.9047619  0.84761905        nan        nan\n",
      "        nan        nan        nan 0.92380952 0.92380952 0.91428571\n",
      "        nan 0.98095238 0.93333333 0.8952381         nan        nan\n",
      "        nan        nan        nan 0.95238095 0.95238095 0.91428571\n",
      "        nan 0.95238095 0.95238095 0.94285714        nan        nan\n",
      "        nan        nan        nan 0.95238095 0.94285714 0.93333333\n",
      "        nan 0.43809524 0.7047619  0.63809524        nan        nan\n",
      "        nan        nan        nan 0.53333333 0.75238095 0.62857143\n",
      "        nan 0.78095238 0.95238095 0.82857143        nan        nan\n",
      "        nan        nan        nan 0.83809524 0.96190476 0.8952381\n",
      "        nan 0.86666667 0.94285714 0.91428571        nan        nan\n",
      "        nan        nan        nan 0.9047619  0.92380952 0.91428571\n",
      "        nan 0.95238095 0.95238095 0.94285714        nan        nan\n",
      "        nan        nan        nan 0.94285714 0.94285714 0.93333333\n",
      "        nan 0.85714286 0.83809524 0.83809524        nan        nan\n",
      "        nan        nan        nan 0.75238095 0.88571429 0.81904762\n",
      "        nan 0.85714286 0.97142857 0.86666667        nan        nan\n",
      "        nan        nan        nan 0.84761905 0.96190476 0.92380952\n",
      "        nan 0.6952381  0.6952381  0.6952381         nan        nan\n",
      "        nan        nan        nan 0.81904762 0.77142857 0.85714286\n",
      "        nan 0.8952381  0.88571429 0.87619048        nan        nan\n",
      "        nan        nan        nan 0.95238095 0.99047619 0.98095238\n",
      "        nan 0.79047619 0.83809524 0.68571429        nan        nan\n",
      "        nan        nan        nan 0.66666667 0.86666667 0.5047619\n",
      "        nan 0.87619048 0.99047619 0.84761905        nan        nan\n",
      "        nan        nan        nan 0.84761905 0.99047619 0.97142857\n",
      "        nan 0.84761905 0.83809524 0.84761905        nan        nan\n",
      "        nan        nan        nan 0.93333333 0.87619048 0.93333333\n",
      "        nan 0.96190476 0.99047619 0.99047619        nan        nan\n",
      "        nan        nan        nan 0.94285714 0.95238095 0.93333333\n",
      "        nan 0.6        0.64761905 0.40952381        nan        nan\n",
      "        nan        nan        nan 0.58095238 0.73333333 0.65714286\n",
      "        nan 0.86666667 1.         0.80952381        nan        nan\n",
      "        nan        nan        nan 0.92380952 0.97142857 0.93333333\n",
      "        nan 0.93333333 0.9047619  0.88571429        nan        nan\n",
      "        nan        nan        nan 0.80952381 0.93333333 0.97142857\n",
      "        nan 0.92380952 0.96190476 0.92380952        nan        nan\n",
      "        nan        nan        nan 0.94285714 0.91428571 0.94285714\n",
      "        nan 0.4        0.7047619  0.42857143        nan        nan\n",
      "        nan        nan        nan 0.43809524 0.7047619  0.4952381\n",
      "        nan 0.83809524 0.98095238 0.95238095        nan        nan\n",
      "        nan        nan        nan 0.9047619  0.98095238 0.92380952\n",
      "        nan 0.55238095 0.9047619  0.65714286        nan        nan\n",
      "        nan        nan        nan 0.66666667 0.9047619  0.65714286\n",
      "        nan 0.88571429 0.97142857 0.9047619         nan        nan\n",
      "        nan        nan        nan 0.87619048 0.93333333 0.87619048\n",
      "        nan 0.74285714 0.7047619  0.7047619         nan        nan\n",
      "        nan        nan        nan 0.71428571 0.85714286 0.73333333\n",
      "        nan 0.82857143 0.78095238 0.81904762        nan        nan\n",
      "        nan        nan        nan 0.91428571 0.98095238 0.94285714\n",
      "        nan 0.6952381  0.6952381  0.6952381         nan        nan\n",
      "        nan        nan        nan 0.78095238 0.74285714 0.74285714\n",
      "        nan 0.74285714 0.6952381  0.7047619         nan        nan\n",
      "        nan        nan        nan 0.96190476 0.94285714 0.96190476\n",
      "        nan 0.68571429 0.65714286 0.68571429        nan        nan\n",
      "        nan        nan        nan 0.61904762 0.74285714 0.59047619\n",
      "        nan 0.74285714 0.76190476 0.81904762        nan        nan\n",
      "        nan        nan        nan 0.91428571 0.95238095 0.93333333\n",
      "        nan 0.73333333 0.6952381  0.73333333        nan        nan\n",
      "        nan        nan        nan 0.93333333 0.83809524 0.82857143\n",
      "        nan 0.82857143 0.75238095 0.79047619        nan        nan\n",
      "        nan        nan        nan 0.94285714 0.97142857 0.93333333\n",
      "        nan 0.39047619 0.73333333 0.52380952        nan        nan\n",
      "        nan        nan        nan 0.36190476 0.67619048 0.57142857\n",
      "        nan 0.83809524 0.79047619 0.8               nan        nan\n",
      "        nan        nan        nan 0.91428571 0.98095238 0.86666667\n",
      "        nan 0.74285714 0.7047619  0.73333333        nan        nan\n",
      "        nan        nan        nan 0.53333333 0.98095238 0.71428571\n",
      "        nan 0.88571429 0.92380952 0.88571429        nan        nan\n",
      "        nan        nan        nan 0.8952381  0.96190476 0.88571429\n",
      "        nan 0.33333333 0.71428571 0.32380952        nan        nan\n",
      "        nan        nan        nan 0.34285714 0.6952381  0.35238095\n",
      "        nan 0.77142857 0.8952381  0.83809524        nan        nan\n",
      "        nan        nan        nan 0.9047619  0.98095238 0.84761905\n",
      "        nan 0.61904762 0.79047619 0.64761905        nan        nan\n",
      "        nan        nan        nan 0.59047619 0.95238095 0.57142857\n",
      "        nan 0.82857143 0.94285714 0.87619048        nan        nan\n",
      "        nan        nan        nan 0.85714286 0.95238095 0.88571429]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = SGDClassifier()\n",
    "\n",
    "tuning_parameters = {\n",
    "    'loss': ['hinge', 'log', 'modified_huber'],\n",
    "    'penalty': ['none', 'l2', 'l1', 'elasticnet'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Hyperparameter Random  Search using an SDG classifier\n",
    "\n",
    "Now, add code to run a `RandomizedSearchCV` instead.\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L09/Figs/randomsearch.png\" alt=\"WARNING: could not get image from server.\"  style=\"width:350px\" >\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of randomized search for two distinct hyperparameters. </center> \n",
    "</em></small>\n",
    "\n",
    "Use these default parameters for the random search, similar to the default parameters for the grid search\n",
    "\n",
    "```python\n",
    "random_tuned = RandomizedSearchCV(\n",
    "    model, \n",
    "    tuning_parameters, \n",
    "    n_iter=20, \n",
    "    random_state=42, \n",
    "    cv=CV, \n",
    "    scoring='f1_micro', \n",
    "    verbose=VERBOSE, \n",
    "    n_jobs=-1\n",
    ")\n",
    "```\n",
    "\n",
    "but with the two new parameters, `n_iter` and `random_state` added. Since the search-type is now random, the `random_state` gives sense, but essential to random search is the new `n_tier` parameter.\n",
    "\n",
    "So: investigate the `n_iter` parameter...in code and write a conceptual explanation  in text.\n",
    "\n",
    "Comparison of time (seconds) to complete `GridSearch` versus `RandomizedSearchCV`, does not necessarily give any sense, if your grid search completes in a few seconds (as for the iris tiny-data). You need a search that runs for minutes, hours, or days.\n",
    "\n",
    "But you could compare the best-tuned parameter set and best scoring for the two methods. Is the random search best model close to the grid search?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEARCH TIME: 0.21 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'optimal', 'eta0': 0.01, 'alpha': 0.001}\n",
      "\tbest 'f1_micro' score=0.8666666666666666\n",
      "\tbest index=19\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.001, eta0=0.01, loss='modified_huber')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: nan (+/-nan) for {'penalty': 'none', 'loss': 'modified_huber', 'learning_rate': 'adaptive', 'eta0': 0.1, 'alpha': 0.1}\n",
      "\t[ 1]: nan (+/-nan) for {'penalty': 'none', 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 1, 'alpha': 0.001}\n",
      "\t[ 2]: nan (+/-nan) for {'penalty': 'none', 'loss': 'hinge', 'learning_rate': 'constant', 'eta0': 0.1, 'alpha': 0.1}\n",
      "\t[ 3]: nan (+/-nan) for {'penalty': 'l1', 'loss': 'log', 'learning_rate': 'optimal', 'eta0': 1, 'alpha': 0.1}\n",
      "\t[ 4]: nan (+/-nan) for {'penalty': 'l2', 'loss': 'log', 'learning_rate': 'adaptive', 'eta0': 0.1, 'alpha': 0.01}\n",
      "\t[ 5]: nan (+/-nan) for {'penalty': 'none', 'loss': 'hinge', 'learning_rate': 'optimal', 'eta0': 0.01, 'alpha': 0.001}\n",
      "\t[ 6]: 0.695 (+/-0.047) for {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'constant', 'eta0': 1, 'alpha': 0.0001}\n",
      "\t[ 7]: nan (+/-nan) for {'penalty': 'none', 'loss': 'hinge', 'learning_rate': 'constant', 'eta0': 10, 'alpha': 0.001}\n",
      "\t[ 8]: nan (+/-nan) for {'penalty': 'none', 'loss': 'log', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 0.01}\n",
      "\t[ 9]: nan (+/-nan) for {'penalty': 'none', 'loss': 'log', 'learning_rate': 'constant', 'eta0': 10, 'alpha': 0.0001}\n",
      "\t[10]: nan (+/-nan) for {'penalty': 'elasticnet', 'loss': 'log', 'learning_rate': 'adaptive', 'eta0': 0.1, 'alpha': 0.1}\n",
      "\t[11]: nan (+/-nan) for {'penalty': 'none', 'loss': 'modified_huber', 'learning_rate': 'optimal', 'eta0': 0.01, 'alpha': 0.001}\n",
      "\t[12]: nan (+/-nan) for {'penalty': 'elasticnet', 'loss': 'log', 'learning_rate': 'constant', 'eta0': 0.01, 'alpha': 0.001}\n",
      "\t[13]: 0.819 (+/-0.111) for {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'invscaling', 'eta0': 0.1, 'alpha': 0.001}\n",
      "\t[14]: nan (+/-nan) for {'penalty': 'none', 'loss': 'log', 'learning_rate': 'adaptive', 'eta0': 10, 'alpha': 0.1}\n",
      "\t[15]: nan (+/-nan) for {'penalty': 'none', 'loss': 'modified_huber', 'learning_rate': 'optimal', 'eta0': 10, 'alpha': 0.001}\n",
      "\t[16]: 0.819 (+/-0.258) for {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'optimal', 'eta0': 1, 'alpha': 0.01}\n",
      "\t[17]: 0.810 (+/-0.148) for {'penalty': 'l2', 'loss': 'hinge', 'learning_rate': 'invscaling', 'eta0': 0.1, 'alpha': 0.01}\n",
      "\t[18]: nan (+/-nan) for {'penalty': 'none', 'loss': 'log', 'learning_rate': 'invscaling', 'eta0': 0.01, 'alpha': 0.1}\n",
      "\t[19]: 0.867 (+/-0.258) for {'penalty': 'l2', 'loss': 'modified_huber', 'learning_rate': 'optimal', 'eta0': 0.01, 'alpha': 0.001}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.79      1.00      0.88        11\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.001, eta0=0.01, loss='modified_huber')\n",
      "\n",
      "best: dat=iris, score=0.86667, model=SGDClassifier(alpha=0.001,eta0=0.01,learning_rate='optimal',loss='modified_huber',penalty='l2')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "75 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of SGDClassifier must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of SGDClassifier must be a str among {'elasticnet', 'l1', 'l2'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of SGDClassifier must be a str among {'l2', 'l1', 'elasticnet'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of SGDClassifier must be a str among {'l2', 'elasticnet', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of SGDClassifier must be a str among {'elasticnet', 'l2', 'l1'} or None. Got 'none' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'modified_huber', 'log_loss', 'perceptron', 'epsilon_insensitive', 'squared_hinge', 'squared_epsilon_insensitive', 'squared_error', 'huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'squared_epsilon_insensitive', 'hinge', 'squared_hinge', 'log_loss', 'modified_huber', 'perceptron', 'huber', 'squared_error'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'modified_huber', 'squared_hinge', 'perceptron', 'epsilon_insensitive', 'huber', 'squared_error', 'log_loss', 'squared_epsilon_insensitive'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'huber', 'squared_hinge', 'squared_error', 'log_loss', 'modified_huber', 'epsilon_insensitive', 'hinge', 'perceptron', 'squared_epsilon_insensitive'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'squared_epsilon_insensitive', 'huber', 'squared_error', 'perceptron', 'squared_hinge', 'epsilon_insensitive', 'hinge', 'modified_huber'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'squared_epsilon_insensitive', 'squared_error', 'perceptron', 'epsilon_insensitive', 'modified_huber', 'log_loss', 'huber', 'hinge'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_epsilon_insensitive', 'squared_hinge', 'perceptron', 'log_loss', 'modified_huber', 'squared_error', 'huber', 'epsilon_insensitive'}. Got 'log' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'modified_huber', 'hinge', 'huber', 'squared_hinge', 'squared_epsilon_insensitive', 'epsilon_insensitive', 'perceptron', 'squared_error'}. Got 'log' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      " 0.6952381         nan        nan        nan        nan        nan\n",
      "        nan 0.81904762        nan        nan 0.81904762 0.80952381\n",
      "        nan 0.86666667]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "random_tuned = RandomizedSearchCV(\n",
    "    model, \n",
    "    tuning_parameters, \n",
    "    n_iter=20, \n",
    "    random_state=42, \n",
    "    cv=CV, \n",
    "    scoring='f1_micro', \n",
    "    verbose=VERBOSE, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time()\n",
    "random_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(random_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd MNIST Search Quest II\n",
    "\n",
    "Finally, a search-quest competition: __who can find the best model+hyperparameters for the MNIST dataset?__\n",
    "\n",
    "You change to the MNIST data by calling `LoadAndSetupData('mnist')`, and this is a completely other ball-game that the iris _tiny-data_: it's much larger (but still far from _big-data_)!\n",
    "\n",
    "* You might opt for the exhaustive grid search, or use the faster but-less optimal random search...your choice. \n",
    "\n",
    "* You are free to pick any classifier in Scikit-learn, even algorithms we have not discussed yet---__except Neural Networks and KNeighborsClassifier!__. \n",
    "\n",
    "* Keep the score function at `f1_micro`, otherwise, we will be comparing 'æbler og pærer'. \n",
    "\n",
    "* And, you may also want to scale your input data for some models to perform better.\n",
    "\n",
    "* __REMEMBER__, DO NOT USE any Neural Network models. This also means not to use any `Keras` or `Tensorflow` models...since they outperform most other models, and there are also too many examples on the internet to cut-and-paste from!\n",
    "\n",
    "Check your result by printing the first _return_ value from `FullReport()` \n",
    "```python \n",
    "b1, m1 = FullReport(random_tuned , X_test, y_test, time_randomsearch)\n",
    "print(b1)\n",
    "```\n",
    "that will display a result like\n",
    "```\n",
    "best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n",
    "```\n",
    "and paste your currently best model into the message box, for ITMAL group 09 like\n",
    "```\n",
    "Grp09: best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n",
    "\n",
    "Grp09: CTOR for best model: SGDClassifier(alpha=1.0, average=False, class_weight=None, early_stopping=False,\n",
    "              epsilon=0.1, eta0=0.0001, fit_intercept=True, l1_ratio=0.15,\n",
    "              learning_rate='invscaling', loss='hinge', max_iter=1000,\n",
    "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
    "              random_state=None, shuffle=True, tol=0.001,\n",
    "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "```\n",
    "              \n",
    "on Brightspace: \"L09: Regularisering, optimering og søgning\" | \"Qd MNIST Search Quest\"\n",
    "\n",
    "> https://itundervisning.ase.au.dk/itmal_quest/index.php\n",
    "\n",
    "and, check if your score (for MNIST) is better than the currently best score. Republish if you get a better score than your own previously best. Deadline for submission of scores is the same as the deadline for the O3 journal handin.\n",
    "\n",
    "Remember to provide an ITMAL group name manually, so we can identify a winner: the 1. st price is  cake! \n",
    "\n",
    "For the journal hand-in, report your progress in scoring choosing different models, hyperparameters to search and how you might need to preprocess your data...and note, that the journal will not be accepted unless it contains information about Your results published on the Brightspace 'Search Quest II' page!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:(in code and text..)\n",
    "assert False, \"participate in the Search Quest---remember to publish your result(s) on Brightspace.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-\n",
    "2018-03-01| CEF, initial.\n",
    "2018-03-05| CEF, updated.\n",
    "2018-03-06| CEF, updated and spell checked.\n",
    "2018-03-06| CEF, major overhaul of functions.\n",
    "2018-03-06| CEF, fixed problem with MNIST load and Keras.\n",
    "2018-03-07| CEF, modified report functions and changed Qc+d.\n",
    "2018-03-11| CEF, updated Qd.\n",
    "2018-03-12| CEF, added grid and random search figs and added bullets to Qd.\n",
    "2018-03-13| CEF, fixed SVC and gamma issue, and changed dataload to be in fetchmode (non-keras).\n",
    "2019-10-15| CEF, updated for ITMAL E19\n",
    "2019-10-19| CEF, minor text update.\n",
    "2019-10-23| CEF, changed demo model i Qd) from MLPClassifier to SVC.\n",
    "2020-03-14| CEF, updated to ITMAL F20.\n",
    "2020-10-20| CEF, updated to ITMAL E20.\n",
    "2020-10-27| CEF, type fixes and minor update.\n",
    "2020-10-28| CEF, added extra journal hand-in specs for Search Quest II, Qd.\n",
    "2020-10-30| CEF, added non-use of KNeighborsClassifier to Search Quest II, Qd.\n",
    "2020-11-19| CEF, changed load_mode=2 (Keras) to load_mode=0 (auto) for MNIST loader.\n",
    "2021-03-17| CEF, updated to ITMAL F21.\n",
    "2021-10-31| CEF, updated to ITMAL E21.\n",
    "2021-11-05| CEF, removed iid=True paramter from GridSearchCV(), not present in current version of Scikit-learn (0.24.1).\n",
    "2022-03-31| CEF, updated to SWMAL F22.\n",
    "2022-08-30| CEF, updating to v1 changes.\n",
    "2022-11-04| CEF, updated link to Brightspace, Search Quest II.\n",
    "2022-11-04| CEF, fixed error \"TypeError: classification_report() takes 2 position..\".\n",
    "2022-11-11| CEF, elaborated on Search Quest II deadline.\n",
    "2023-03-24| CEF, updated link and updated to SWMAL F23."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
